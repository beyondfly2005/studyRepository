# 黑马程序员 全网最全的大数据入门教程

> 课程地址   https://www.bilibili.com/video/BV155411J7Wj

### 第一章 大数据概论

#### P1 、课程学习目标和内容概要

课程内容

- 大数据概览和职业规划
- Linux服务器系统
- Hadoop概览
- HDFS分布式文件系统
- Hive数据仓库
- SparSQL指令
- Zeppeline框架
- Sqoop框架
- Superset数据可视化
- 大数据数仓实战-滴滴出行

大数据概览

- 职业相关

- Linux系统

- Hadoop概览

- HDFS分布式文件系统

- Hive数仓

- Hive架构

- Zinplin框架

- Sqoop框架 

  HDFS文件系与关系型数据库的转换

- SuperSet框架

  展现

- 滴滴出行项目

  Spark+Hive+HDFS+Zeeplin+Sqoop+MySQL+Superset可视化

#### P2、第一章大数据概论 内容介绍

- 大数据概念
- 大数据特点
- 大数据应用场景
- 大数据分析业务步骤
- 大数据职业规划
- 大数据学习路线

#### P3、大数据概念

什么 是大数据

- 什么是数据
- 大数据定义
- 大数据出出力数据量有多大

| 1Byte    | 8bit     | |
| -------- | -------- | |
| 1K（千） | 1024Byte | |
| 1MB      |          | |
| 1G       |          | |
| 1T       |          | |
| 1P       |          | 大数据处理 |
| 1E       |          | 大数据处理 |
| 1Z       |          | |
| 1Y       |          | |
| 1B       |          | |
| 1N       |          | |
| 1D       | 1024N    | |



- 大数据解决什么问题

  **海量数据存储**

  **海量数据运算**

#### P4、大数据的特点

大数据的特点

5V：大、多、值、快、信

- 数据体量大

- 种类和来源多样化

- 低价值密度

- 速度快

  数据增长速度快

  处理速度快

  获取数据的速度快

- 数据的质量

  数据的准确性

  数据的可信赖度

P5、大数据的而应用场景

##### 行业领域

金融、互联网、制造业、公共管理、交通与物流

##### 金融借贷

大数据让金融借贷更放心

##### 广告营销

让广告营销更高效

##### 新媒体/短视频

大数据让新媒体更懂你

#### P6、大数据分析业务步骤

###### 分析流程

- 明确数据分析的目的和思路
- 数据收集（Sqoop、flume）
- 数据处理：提取、清洗、转换、加载（Sqook、MapReduce）
- 数据分析：统计、建模、挖掘（Hvie、Spark、Flink）
- 数据可视化（Superset、Echarts、BI工具）
- 报告撰写

#### P7、大数据职业规划

##### 岗位需求

- 大数据开发工程师
- Hadoop开发工程师
- Spark开发工程师
- 实时计算开发工程师
- 数据仓库工程师
- ETL开发工程师
- BI工程师
- 数据挖掘工程师
- 数据架构师

主要分为：流失计算（流式计算）和 离线计算

##### 薪资水平

一线城市：北京30-50K

二线城市：

三线城市：12-18K

##### 职业规划

- 大数据开发工程师
- 高级大数据开发工程师（数据挖掘）
- 大数据架构师
- 大数据技术总监

##### 岗位技术要求

- Java  Shell Python
- Hadoop HDFS MapResuce Kafka Hive HBase Spark OLAP Kylin

#### P8、学习路线

- Linux系统

- 编程语言

  Java  SQL Scala Python

  Java：JavaSE 基础语法 集合 IO 线程 网络

  SQL：

  Scala：Spark使用的Scala使用的语言

  Python：机器学习使用的语言

- 大数据框架

- 核心框架：Hoop、Hive、Spark、Flink、Kafka、Hbase

- 实际项目：离线数据分析（）

- 实际项目：实时数据分析（车联网）

### 第二章 Linux服务器系统

#### P9、本章内容介绍

内容

- 计算机入门知识介绍
- Linux系统介绍
- Linux系统的安装和体验
- Linux的网络配置和连接工具
- Linux的目录结构
- Linux的常用命令
- vi编辑器

#### P10、计算机入门知识-硬件和软件

#### P30、Linux常用命令-文件操作命令-cp命令

#### P31、Linux常用命令-系统管理命令-ps命令

#### P32、Linux常用命令-系统管理命令-ps命令

###### 作用

ps命令用来累出系统中当前运行的进程

###### 格式

ps [参数]

###### 案例

```bash
ps

ps -ef
# 显示全部进程
```

#### P33、Linux常用命令-系统管理命令-kill命令

###### 作用

kill命令用于终止执行中的程序

###### 格式

kill [参数] [进程号]

```bash
kill -9 9437
```

-9 是信号的编号

```bash
kill -l
# 列出所有信号
kill -SIGKILL 9437 
# 使用信号的名称 杀死进程
```

#### P34、26-Linux常用命令-网络管理命令-hostname命令

- ##### 作用

  hostname 命令用来查看主机名

- 格式

  ```bash
  hostname
  ```

- 

##### P35、27-Linux常用命令-网络管理命令-ifconfig命令

- ###### 作用

  ifconfig命令用来查看ip地址

- ###### 格式

  ifconfig

- 案例

```bash
ifconfig
```

#### P36、28-Linux常用命令-网络管理命令-netstat命令

- ###### 作用

  netstat命令用于显示与网络协议相关（TCP/UDP）的统计数据

- 格式

  netstat [参数]

- 案例

  ```bash
  netstat -nltup
  #列出当前计算机占用的TCP/UDP端口信息
  ```

  

#### P37、Linux常用命令-clear命令

- 作用

  清屏

- 格式

  clear

- 案例

  ```
  clear
  ```

  

#### P38、Linux常用命令-重启和关机命令

- 重启命令

  reboot

- 关机命令

  shudown -h now  立刻关机(断电关机)

  halt  立刻关机(不断电关机)

### 第三章 HDFS分布式文件系统

#### P43、本章内容介绍

- 分布式系统和集群
- Hadoop框架概论
- HDFS文件系统

#### P44、分布式系统概念

- 概念

  分布式是将多台服务器集中在一起，每台服务器都实现总体中不同的业务，做不同的事情

- 单机模式

- 分布式模式

#### P45、集群的概念

- 概念

  所谓集群是指一组独立的计算机系统构成的一个多处理器系统，他们之间通过网络实现进程间的通信，让若干计算机联合起来工作(服务)，可以是并行的，也可以是做备份

- A

- 分布式和集群的区别

  分布式：分布式的主要工作是分解任务，将职责拆解，多个人在一起做不同的事

  集群：集群主要是将同一个业务，部署在多个服务器上，多个人在做同样的事情

#### P46、Hadoop概论-Hadoop介绍

Hadoop是Apache旗下的一个用Java语言实现的开源软件架构，是一个存储和计算大规模数据的软件平台。

Hadoop是Apache Lucene创世人Doug Cuttiong创建的，最早起源一个Nutch项目

#### P47、Hadoop概论-框架内容

##### 狭义解释

Hadoop指的Apache这块开源框架，它的核心组件有：

- HDFS分布式文件系统
- MapReduce分布式编程运算框架

##### 广义解释

Hadoop生态

Zookeeper、Hive Redis YARN Hbase Sqoop Flume HDFS Zepelin Flink

必须学习

- Kafka
- Spark
- Hive
- Flink
- MapReduce

#### P48、Hadoop概论-国内外应用

国外应用

Yahoo

- 广告系统
- 用户行为分析

国内应用

阿里巴巴

- 数据平台系统
- 搜索引擎
- 电子商务平台
- 搜索

#### P49、07-Hadoop概论-Hadoop的版本

通用版本

- 1.x
- 2.x版本系列：架构城市重大变化，引入Yarn平台等许多新特性，是现在使用的主流版本
- 3.x

发行版

开源社区版和商业版

- 开源社区版本
- 商业版  clouderaCDH

2.7.5

#### P50、08-Hadoop概论-框架内容

Hadoop架构模块

- HDFS
- MapReduce
- YERN

Hadoop2.x架构内部模型

![img](https://img-blog.csdn.net/20180914170641766?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjI5NTE0MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



HDFS模块

- NameNode





Hadoop2.x架构模型-MapReduce



Hadoop模块之间的关系

- MapReduce计算需要的数据和产生的结果需要HDFS进行存储
- MapReduce的运算需要由Yarn集群来提供资源调度

#### P51、09-Hadoop集群搭建方案

###### 集群简介

HDFS集群和YARN集群

###### HDFS集群

NameNode DataNode SecondaryNameNode

###### YARN集群

ResourceManager NodeManager

###### 集群搭建方式

- 单机模式，一台集群上运行HDFS的NameNode和DataNode

- 集群模式

###### 大数据控集群方案-单机模式

搭建号Hadoop所有开发环境，方案如下

| 框架 | 组件 | 是否安装 |
| ---- | ---- | -------- |
|HDFS框架 | NameNode     |     是     |
|HDFS框架 | Secondary NameNode | 是|
|HDFS框架 | DataNode | 是|
|Yarn框架 | ResourceManager | 是|
|Yarn框架 | NodeManager | 是|
|Hive框架 |Hive框架 | 是 |
|Spark框架 |Spark框架 | 是 |
|Zeppelin框架 |Zeppelin框架 | 是 |
|Superset框架 |Superset框架 | 是 |

单机模式下，要求windows系统运行内存至少是8G

集群模式

| 框架 | 组件 | Node1 | Node2 | node3 |
| ---- | ---- | ----- | ----- | ----- |
| HDFS |      |       |       |       |
| HDFS |      |       |       |       |
| HDFS |      |       |       |       |
|      |      |       |       |       |

集群模式下 要求windows系统的运行内存至少是16G

#### P52、10-Hadoop集群操作-单机模式-启动和关闭

##### 启动和关闭

###### 1 启动虚拟机

2 连接虚拟机

3、集群一键启动和关闭

- 一键启动大数据环境

```bash
cd /export/onkey
./start-all.sh
```

4、查看启动进程-jps	

```bash
jps  	# 查看java相关的进程
```

5、关闭

```
./stop-all.sh
```

5、查看HDFS页面

```
http://ip:50070/
htpp://192.168.1.
```

#### P53、11-Hadoop集群操作-单机模式-初体验

```
cd /root
hadoop fs -put xx.txt /
```

通过页面查看上传是否成功

MapReduce使用

计算圆周率

```bash
hadoop jar /export/server/hadoop-2.7.5/mapreduce/hadoop-mapreduce-example-2.7.5.jar pi 2 10
```

#### P54、12-Hadoop集群操作-集群模式-启动和关闭

###### 1- 启动三台虚拟机

###### 2-分别连接三台虚拟机

###### 3-集群一键启动和关闭

```bash
cd /export/onekey/
./start-all.sh
```

###### 4-查看进程-jsp

```bash
jps
```

###### 5-一键关闭

```
./stop-all.sh
```

6-查看

```
http://ip:50070/
http://192.168.1.100:50070/
```

进入hdfs页面

#### P55、Hadoop集群操作-集群模式-初体验

##### HDFS的使用

```bash
cd /root
hadoop fs -put xxfile /		#上传到根目录
```

MapReduce使用

计算圆周率

```bash
hadoop jar /export/server/hadoop-2.7.5/mapreduce/hadoop-mapreduce-example-2.7.5.jar pi 2 10
```

#### P56、HDFS文件系统-HDFS概述

##### HDFS的概述

- 在现代的企业环境中，单机容量往往无法存储大量数据
- HDFS(Hadoop Distributed File System) 是Apache Hadoop项目的一个子项目。Jadoop非常适合存储大型数据（TB和PB）
- 分布式文件解决的问题件就是大数据的存储，他们是横跨在多台计算机上的存储系统，分布式文件系统在大数据时代有着广泛的应用前景，他们为存储和处理超大规模数据提供所需的扩展能力。

#### P57、15-HDFS文件系统-HDFS特点

###### HDFS的特点

- HDFS系统可存储超大文件，时效性稍差
- HDFS具有硬件检测和自动快速恢复功能
- HDFS为数据存储提供很强的扩展能力
- HDFS存储一般为一次写入，多次读取，只支持追加写入，不支持随机修改
- HDFS

#### P58、16-HDFS文件系统-HDFS架构

##### HDFS的架构

- HDFS采用Master/Slave架构
- 一个HDFS集群有两个重要的角色，分别是NameNode和DataNode
- HDFS的四个基本组件：HDFS Client、NameNode、DataNode、Secondary NameNode

##### HDFS Client

- 客户端
- 文件切分，文件上传HDFS的时候，Client将文件切分为一个个Block（128M），然后进行存储
- 与NameNode交换，获取文件的位置信息
- 与DataNode交换，读取或者写入数据
- Client提供一些命令来管理和访问HDFS，比如启动或关闭HDFS

##### NameNode

- 就是master 台式一个主管、管理者

- 管理HDFS元数据（文件路径，文件的大小，文件的名字，文件的权限、文件的Block切片信息）

  元数据信息存储在内置中 约150字节

- 配置副本策略

- 处理客户端读写请求

###### DataNode

- 就是Slave。NameNode下达命令，DataNode执行实际的操作
- 存储时间的数据块
- 执行数据块的读/写操作\
- 定时向NameNode汇报block信息

##### Secondary NameNode

- 并非NameNode的热备份。当NameNode关掉的时候，他并不能马上替换NameNode并提供服务
- 负载NameNode，分带其工作量（秘书）
- 在紧急情况下下，可辅助恢复NameNode

#### p59、17-HDFS文件系统-HDFS副本机制

##### HDFS的副本机制

- HDFS被设计成能够在一个大集群中跨集群可靠的存储超大文件，它将每个文件存储为一些列的数据块，这个数据库被称为bloack
- 为了容错，文件的所有block都会有副本，每个文件的数据块打下耦合副本系数都是可配置的
- hadoop2中，文件的block块大小默认是128M（134217728字节）

#### P60、18-HDFS的Shell命令-介绍

##### Shell命令介绍

- 对HDFS的操作命令类似于Linux的shell对文件的操作，如ls mkdir rm等



Hadoop提供了文件系统的shell命令使用格式如下

```bash
hadoop fs <args> #既可以操作HDFS，也可以操作本地系统
#或者
hdfs dfs <args> #只能操作HDFS系统
```

> 本课程使用上面一种格式

#### P61、HDFS的Shell命令-ls命令

ls命令

格式：hadoop fs -ls URI

作用：类似Linux系统的ls命令，显示文件列表

示例：

```
hadoop fs -ls /
```

#### P69、27-HDFS的基准测试

HDFS的基准测试

实际生成环境中

- 写入速度测试

  向布该HDFS系统中写入数据

  ```
  hadoop /export/server/hadoop/
  ```

  



### 第四章 Hive实战和Zeppelin框架

#### P70、本章内容介绍

主要内容

- 数据仓库概论
- Hive概论
- Hive基本操作
- Zeppelin框架
- Hive查询操作
- Hive内置函数

#### P71、数据仓库基本概念

数据仓库的基本概念

数据仓库，英文名称为Data Warehouse 可简写为DW或DWH。

数据仓库故名思夜，是一个很大的数据存储集合，出于企业的分析报告和决策支持目的而创建。他为企业提供了一定的BI(商业智能)

#### P72、数据库和数据仓库区别

数据库和数据仓库的区别

###### 数据库

数据库是面向交易的处理系统，它是指很对具体业务在数据库联机的日常操作，通常对记录进行查询、修改。用户较为关心操作的响应时间，数据的安全性，并发支持的用户数量等。

传统的数据库系统作为数据关联的主要手段，主要用于操作型处理

###### 数据仓库

数据仓库异步针对某些主题的历史数据进行分析，支持管理决策，又被称为连接分析处理OLAP(On-Line Analytical Processing)

首先要命名，数据仓库的出现并不是要取代数据库

支付宝年度账单

###### 数据库和数据仓库的区别

- 数据库是面向事务的设计，数据仓库是面向主题设计的
- 数据库异步存储业务数据，舒克仓库存储的一般是历史数据
- 数据库是为捕获数据而设计的

#### P73、数据仓库分层

数据仓库分层

安装数据流入流出的谷草，数据仓库架构可以分为三层：元数据ODS、数据仓库DW、数据应用APP

#### P74、Hive的介绍

##### 什么是Hive

Hive是一个构建在Hadoop上的数据仓库框架，最初，Hive是由Facebook开发的，后来移交Apache软件基金会

##### Hive的特点

- Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库，并提供类似SQL查询功能
- Hive能够存储很大的数据集，他可以直接访问粗糙农户在Apache HDFS或者其他数据存储系统（如Hbase）中的文件
- Hive支持MapReduce、Spark、Tez这三种分布式计算引擎

##### P75、Hive的基本架构

Hive是建立在Hadoop上的数据仓库基础架构。它提供了一系列的工具，可以存储，查询和分析存储在分布式是存储熊中的大规模数据集，Hive定义了

![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fstatic.zybuluo.com%2Fwangcao%2F0tg7epdsdag0isjlvadjmsyb%2F1.png&refer=http%3A%2F%2Fstatic.zybuluo.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1613886068&t=d5cd6cab17bedb6bfed91f42e35073e3)

#### P75、Hive的计算引擎



##### MapReduce

它将计算分为两个阶段，分别为Map和Reduce，对于应用来说，需要想方设法将应用拆分成多个map、Reduce作业，以完成一个完整的算法



##### Tez

Tez把Map/Reduce过程拆分为若干个子过程，同时可以把多个Map/Reduce任务组合成为



##### Spark

Apache Spack是一个快速的，多用途的集群计算系统，相当于Hadoop MapReduce

Spark实现了一种叫做RDDs执行引擎，其数据缓存在内存中可以进行迭代处理。

#### P77、08-Hive的安装和启动

###### Hive的安装

本套课程中使用的是Hive+Spack

1-启动集群中的

```
cd /export/onkey
./start-all
```

2-使用ssh终端连接Hive

```
1-进入到 /export/server/export/spark-2.3-bin-hadoop2.7/bin 目录中
2-执行命令 ./beeline
3-输入 !connect jdbc:hive2://node1:10000  回车
4-输入用户名 root
5-直接回车 ，即可使用命令行模式连接到Hive ，然后既可以执行HQL了
```

启动hive



```sql
show databases;
```

#### P78、Hive的数据库和表

Hive的数据库和表

- Hive数仓和传统关系型数据库类似，管理数仓数据也是通过数据库和表

#### P79、10-Hive的数据库操作-创建数据库

##### 创建数据库-默认方式

```sql
create database if not exists myhive;
show databases;

--# 说明：
1：if not exists: 该参数可选
2：hive的数据库默认放在 /usr/hive/warehouse/myhivedb
```

创建数据库-指定存储路径

```sql
create database myhive2 location '/myhive2';
--# 在根目录下创建数据库
```

#### P80、11-Hive的数据库操作-查看数据库详情和删除数据库

##### 查看数据的详细信息

```sql
desc database myhive;  --myhive 数据名
-- #将会显示
--数据库名、描述、路径
```

##### 删除数据库

删除一个空数据库，如果数据库下面有数据表，就会报错

```sql
drop database myhive;
```

强制删除一个数据库，保护数据库下面的表一起删除

```sql
drop database myhive2 cascade;
```

#### P81、12-Hive的数据表操作-创建表语法和数据类型

##### 创建数据库表

```sql
create [external] table [if not exists] table_name  --external共享
[(column_name data_type] [comment col_comment], ...  )]
[comment table_comment]
```

##### 数据类型

- 原始类型

| 类型      | 描述                     | 示例 |
| --------- | ------------------------ | ---- |
| BOOLEAN   | true/false               |      |
| TINYINT   |                          |      |
| SMALLINT  |                          |      |
| INT       |                          |      |
| BIGINT    |                          |      |
| FLOAT     | 4字节 单精度浮点数       |      |
| DOUBLE    |                          |      |
| DEICIMAL  | 任意精度的带浮点数的小时 |      |
| String    |                          |      |
| varchar   |                          |      |
| Char      |                          |      |
| Binary    | 字节数组                 |      |
| Timestamp | 时间戳，毫秒值精度       |      |
| Date      | 日期                     |      |
| Time      | 时间 时分秒              |      |
| DateTime  | 日期时间 年月日时分秒    |      |

- 复杂类型

| 分类   | 描述                                            | 字面量示例               |
| ------ | ----------------------------------------------- | ------------------------ |
| Array  | 有序的同类型的集合                              | ["北京","上海","石家庄"] |
| Map    | key-value,key必须为原始类型 value可以是任意类型 |                          |
| Struct | 字段集合，类型可以不同                          | struct('1',1,1.0)        |

#### P82、Hive的数据表操作-内部表-创建表

##### 内部表操作-创建表

未被external修饰的是内部表(managed table)，内部表又称为管理表，内部表不适合用于共享数据

```sql
--创建数据库
create database mytest;
--打开数据库
use mytest;
-- 创建内部表
create table stu(id int,name String);
-- 显示数据库里的所有表
show tables; 
```

创建表之后，Hive会在对应的数据库文件夹下创建对应的表的目录

#### P83、Hive的数据表操作-内部表-查看表结构和删除表

##### 内部表操作-查看表结构/删除表

###### 查看表结构

```sql
desc stu; -- 查看表结构基本信息
desc formatted stu; --查看表结构详细信息
```

###### 删除表

```sql
drop table stu;
show tables; -- 查看所有表
```

#### P84、Hive的数据表操作-内部表-直接插入数据

##### 内部表操作-插入数据 方式一

对于Hive中的表，可以通过insert into 指令向表中插入数据

```sql
use mytest; --选择数据库
create table stu(id int,name string); --创建表

--向表中插入数据
insert into stu values(1,'zhangsan');
insert into stu values(2,'lisi');

select * from stu; -- 查询数据
```

该方式每次插入都会在表目录中生成一个对用的数据文件，不推荐使用。

字段之间使用 分隔符 '\001'

#### P85、16-Hive的数据表操作-内部表-load加载数据

##### 内部表操作-插入数据 方式二 load加载数据

load命令用于将外部数据加载到Hive表中

###### 语法

```sql
load data [local] inpath 'filepath' [overwrite] into table tablename [partition(partcol=val1,partcol2=val2 ...)]

-- 说明：
local 表示从本地文件系统加载，否则是从HDFS加载
```

###### 应用示例1-本地加载

```sql
create table if not exists stu2(id int,name string)
row format delimited fields terminated by '\t';    --指定字段之间的分隔符 为\t  也就是键盘的Tab

--#向表中加载数据
load data local inpath '/export/data/hivedatas/stu.txt' into table stu2;  --local一定要加 表示从本地路径加载 而非从HDFS路径加载
```

###### 应用示例2-HDFS加载

```bash
# 上传文件到hadoop
hadoop fs -mkdir -p /hivedatas
cd /export/data/hivedatas
hadoop fs -put stu.txt /hivedatas/
```



```sql

-- 创建表
create table if not exists stu3 (id int,name string)
row format delimited fields terminated by '\t';

--向表中加载数据

-- 加载数据
load data inpath '/hivedatas/stu.txt' into table stu3;
```

#### P86、Hive的数据表操作-内部表-特点总结

##### Hive内部表特点

###### 元数据的概念

- Hive是建立在hadoop之上的数据仓库，存储在hive里数据时间上就是存在HDFS上，都是以文件的形式存在。
- Hive元数据用来记录数据库和表的特征信息，比如数据库的名字、存储路径、表的名字、字段信息、表文件存储路径等等。
- 本课程中Hive的元数据是保存在MySQL数据库中。

###### 内部表特点

- Hive内部表信息存储默认的文件路径是在/usr/hive/warehourse/databasename.db/tablename/目录
- hive内部表在进行drop操作时，其表中的数据以及表的元数据信息都会被删除
- 内部表一般可以用来做中间表或者临时表

#### P87、Hive的数据表操作-外部表-创建表

###### 外部表-创建

创建表时，使用external关键字修饰则为外部表，外部表数据可用于

```sql
create external table teacher (tid string,tname string)
row formate delimited fields by '\t';

```



#### P88、Hive的数据表操作-外部表-数据加载

##### 加载数据

外部表加载数据也是通过load命令来完成

```
cd /export/
```

```
load data local inpath '/export/' into table student;
load data local inpath '/export/tearch.txt' into tearche overwrite;
```



#### P89、Hive的数据表操作-外部表-特点总结

##### 外部表特点

- 外部表在进行drop操作时，仅会删除元数据 不会删除HDFS上的文件
- 外部表一般用于数据共享表，比较安全

```
drop table
```



#### P90、21-Hive的数据表操作-开发工具-VSCode安装

###### 安装Visual Studio Code

开发Hive的时候，我们经常要编写类SQL，是用一款顺手的编辑器会让我们开发更有效率，所以这里我们使用VS Code来开发HQL

扩展中安装 HiveSQL插件

选择一个文件夹，用来保存HQL

新建文件

VS Code 只能帮助我们编写HQL ，有代码提示，但是不能直接执行，并不能连接到Hive，并不是一个客户端

#### P91、22-Hive的数据表操作-分区表-一级分区表操作

###### 分区表介绍

- 在大数据中，最常用额一种思想就是分而治之，分区时间就是对hdfs文件系统上的独立的文件夹，该文件加下是 该分区所有数据文件

- 分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下
- Hive中可以创建一级分解表，也可以创建多级分区表

###### 创建一级分区表

```sql
create table score(id string,cid string,score int)
partitioned by(month string)
row formated delimited fields terminated by '\t';
```

###### 加载数据

```sql
load data local inpath '/export/score.txt' into table
score partition(month='202006');

-- 
select * from score;

-- 多一个字段socre 是分区字段

-- 没有指定exten 内部分区表
-- 自动创建一个文件夹 month=202006

select * from score where month='202006';

select * from score where month='202007';

select * from score where month='202006' union 
select * from score where month='202007';

select * from score where month='202006' union all
select * from score where month='202007';
```

#### P92、Hive的数据表操作-分区表-多级分区表操作

###### 创建多级分区表

```sql
create table scire2(sid string,cid string ,ssore string) 
partitioned by (year string,month String,day string)
```

###### 查看表结构/分区信息

```sql
desc sore2
```

###### 加载数据

```sql
load data local inpath '' into table score2
partion(year='2020',month='06',day='01');
```

###### 查询数据

```sql
select * from score2  where year='2020'

select * from score2  where year='2020' and month='06'

select * from score2  where year='2020' and month='06' and day='01'
```

#### P93、Hive的数据表操作-分区表-其他操作

##### 查看分区

```sql
show partitions score;
```

##### 添加分区

```sql
--添加一个分区
alter table score add partition(month='202008')

--添加多个分区
alter table score add partition(month='202009') partition(month='202010');
```

###### 删除分区

```sql
alter table score drop partition(month='202010');
```

#### P94、25-Zepplin框架-介绍

##### Zepplin介绍

Apache Zepplin是一款基于Web交换式框架，支持多种语言，Scala、SparkSQL、Markdown，SQL、SHell、Python等

Zeppelin提供了数据分析、数据可视化



#### P95、26-Zepplin框架-安装和使用

###### 安装

已经安装好了，并且和Spark和Hive进行了整合

###### 启动

```
./start-all.sh
```

###### 访问

```
http://ip:8090/
```

创建note

​	选择解释器 Hive2-100，（需要提前配置）

编写SparkSQL

执行

```
按钮 
快捷键 Shift+Enter
```

​		

 

#### P96、27-Zepplin框架-参数设置

我们可以通过设置Zeppline的一些参数

设置字体18

```
设置字体

下载
```



#### P97、28-Hive查询操作-查询语法

###### 基本语法

```
select [distinct] select_expr,select_expr ...
from table_reference
[where where_condition]
[group by col_list]
[having where_condition]
[order by col_list] ...
[limit number]
```

解释

```
select 查询关键字
distinct 去重
from 指定查询的表
where 指定查询条件
group by 分组查询
having 
order 排序条件
```

#### P98、29-Hive查询操作-基本查询操作



#### P99、

#### P100、



#### P101、



##### P102、

#### P103、

#### P104、



#### P112、

##### 内置函数介绍

在SparkSQL中提供了很多的内置函数，或者叫内嵌函数，包括聚合函数、数学函数、字符串函数、转换函数、日期函数、条件函数、表生成函数等等

##### 数学函数

###### round 指定精度取整函数

```sql
select round(3.1415926,4);
```



###### rand

```sql
select rand(); -- 返回0-1之间的随机数
select rand(100);  --指定种子 指定种子之后每次执行结果都一样
```

#### P113、

字符串函数

字符串连接函数concat_ws

```sql
select concat_ws(',','ab','cd','ef');
select concat_ws('-',year,month,day);
```

字符串截取函数substr，substring

语法

```
substr(string A, int start ,int len);
substring(int A,int start ,int len);
```



### 第五章 数仓实战之滴滴出行

#### P119、本章内容介绍

- 项目业务背景
- 日志数据集介绍
- 数据仓库构建
- 数据分区表构建
- 数据预处理
- 订单指标分析
- Sqoop数据导出
- Superset数据可视化

#### P120、项目业务背景-业务介绍

使用以下技术来进行数据分析

- HDFS
- Hive
- Spark
- Zepplin
- Sqoop  分析后的数据进行导出，Sqoop是导出工具
- Suerset 数据可视化

#### P121、项目业务背景-项目架构

##### 架构方案

- 订单量庞大
- 我们使用Sqoop导出分析后的数据

##### 架构图

![image-20210122165043004](C:\Users\beyond\AppData\Roaming\Typora\typora-user-images\image-20210122165043004.png)

#### P122、日志数据集介绍-用户订单日志



#### P123、日志数据集介绍-用户取消订单日志



#### P124、日志数据集介绍-用户支付日志

#### p125、日志数据集介绍-用户评价日志

#### P126、数据仓库构建-数仓分层

| 数据类别       | 数据库名 | 分层名        |
| -------------- | -------- | ------------- |
| 原始日志数据   | ods_didi | 临时存储层ODS |
| 预处理后的数据 | dw_didi  | 数据仓库层    |
| 分析结果数据   | app_didi | 应用层(APP)   |



#### P127、数据仓库构建-创建数据库

根据之前的分析，我们的数仓架构为三次，接下来创建三个数据库，分表用来管理每一次的表数据

```sql
--
create database if not exists ods_didi;

--
create database if not exists dw_didi;

--
create database if not exists app_didi;
```



#### P128、数据分区表构建-创建表

-ods创建用户评价表

```
create table if not exists ods_didi.t_user_evaluate

```



#### P129、数据分区表构建-表加载数据



#### P130、数据预处理-预处理需求

##### 预处理需求

- 1- 过滤掉order_time长度小于8的的数据，如果小于8，表示这条数据不合法，不应该参与统计
- 2-将一些0、1表示的字段，初拉力为更容易理解的字段，例如 subscribe字段 0表示非预约 1表示预约
- 3-order_time 字段为2020-4-12 1:15 将字段长度改为一致的格式
- 4-为了方便 按年月日 小时统计
- 5-



#### P131、数据预处理-宽表创建和数据加载

dw层创建宽表

在进行预处理之前，先要把预处理之后保存数据的表创建出来，它包含以下字段

增加了

| 字段名         | 说明                       |
| -------------- | -------------------------- |
| subscribe_name | 是否预约名称               |
| 预约日期       | 对原有预约日期做了格式规范 |
|                |                            |

预处理SQL数据

#### P132、14-订单指标分析-订单总笔数mp4

##### 1- 编写HQL

```sql
select count(orderid) as total_cnt
from dw_didi.t_user_order_wide
where dt='020-04-12';
```

##### 2- App层建表

保存以下字段

- 订单时间（哪天的订单）
- 订单笔数

```sql
create table if not exists app_didi.t_order_total(
	data string coment '日期（年月日）',
    count integer comment '订单笔数'
)
partitioned by(month string comment '年月 yyyy-MM')
row format delimited fields terminated by ','
;
```

##### 3- 加载数据到app表

将查询后的数据 插入到

```sql
insert overwrite table app_didi.t_order_total partition(month='2020-04')
select '2020-04-12' ,count(orderid) as total_cnt
from dw_didi.t_user_order_wide
where dt='2020-04-12';
```

查询表的数

```sql
select * from app_didi.t_order_total
```

#### P133、15-订单指标分析-预约和非预约订单占比

##### 1- 编写HQL语句

```sql
select '2020-04-12',subscribe_name,cont(*) as order_cnt
from dw_didi.t_user_orer_wide
where dt='2020-04-12'
group by subscribe_name
```

##### 2-App层建表

包含三个字段：日期、是否预约、订单数

```sql
create table if not exists app_didi.t_order_subscribe_total(
	date string comment '日期',
	subscribe_name string comment '是否预约',
	count integer comment '订单数据'
)
partitioned by (month string comment '年月，yyyy-MM')
row formate delimited fields terminated by ','
```

##### 3-加载数据到app表

```sql
insert overwrite table app_didi.t_order_subscribe_total partation(month='2020-04')
select '2020-04-12',subscribe_name,cont(*) as order_cnt
from dw_didi.t_user_orer_wide
where dt='2020-04-12'
group by subscribe_name
```

#### P134、16-订单指标分析-不同时段订单占比

##### 1-编写HQL

```sql
select order_time_range,count(*) as order_cnt
from dw_didi.t_user_order_wide
where dt='2020-04-12'
group by order_time_range;
```

##### 2-App层建表

```sql
create table if exists app_didi.t_order_timerange_total (
	date String comment '日期',
    timerange String comment '时间段',
    count integer comment '订单数量'
)
partitioned by (month string comment '年月，yyyy-MM')
row formate delimited fields terminated by ','
```

##### 3-加载数据到App表

```sql
insert overwrite table app_didi.t_order_timerange_total partition(month='2020-04')
select '2020-04-12',order_time_range,count(*) as order_cnt
from dw_didi.t_user_order_wide
where dt='2020-04-12'
group by order_time_range;
```

#### P135、17-订单指标分析-不同地域订单占比

##### 1- 编写HQL

```sql
select province,count(*) as order_cnt
from dw_didi.t_user_order_wide
where dt='2020-04-12'
group by province 
```

##### 2-App层建表

```sql
create table if exists app_didi.t_order_province_total (
	date String comment '日期',
	province String comment '省份',
 	count integer comment '订单数量'
)
partitioned by (month string comment '年月，yyyy-MM')
row formate delimited fields terminated by ','
```

##### 3-加载数据到App表

```sql
insert overwrite table app_didi.t_order_province_total  partition(month='2020-04')
select '2020-04-12',province,count(*) as order_cnt
from dw_didi.t_user_order_wide
where dt='2020-04-12'
group by province 
```

#### P136、18-订单指标分析-不同年龄段和不同时段订单占比

##### 1- 编写HQL

```sql
select age_range,order_time_range,count(*)
from dw_didi.t_user_order_wide
group by age_range,order_time_range;
where dt='2020-04-12'
--多分组 多条件一致完全一致 才会分到一个组
```

##### 2-App层建表

```sql
create table if exists app_didi.t_order_age_and_time_range_total (
	date String comment '日期',
    age_range integer comment '年龄段',
    order_time_range integer comment '时间段'
    count integer comment '订单总数'
)
partitioned by (month string comment '年月，yyyy-MM')
row formate delimited fields terminated by ','
```

##### 3-加载数据到App表

```bash
insert overwrite table app_didi.t_orderr_age_and_time_range_total
select '2020-04-12',age_range,order_time_range,count(*)
from dw_didi.t_user_order_wide
group by age_range,order_time_range;
where dt='2020-04-12'
```

#### P137、19-Sqoop数据导出-Sqoop介绍

##### Sqoop介绍

Apache Scoop是在Hadoop生态系统和RDBMS体系之间产送数据的一种工具

Hadoop生态系统包括：HDSF、Hive、Hbase

RDMS体系包括：MySQL、Oracle、DB2

Sqoop可以理解为“SQL到Hadoop和Hadoop到SQL”

![img](https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412130640231-449939615.png)

#### P138、20-Sqoop数据导出-Sqoop安装和验证

集群环境中已经1.4.7

验证启动

```bash
cd /export/server/sqoop-1.4.7
bin/sqoop list-database \
--connect jdbc:mysql://hadoop1:3306/ \
 --username root \
 --password 123456
 
 # 会显示mysql中存在的数据库
```

#### P139、21-Sqoop数据导出-Mysql创建目标数据库和目标表

##### MySQL中创建目标表

```mysql
#创建目标数据库
create database if not exists app_didi;

#创建订单总笔数 目标表
create table if not exists app_didi.t_order_total(
    order_date date,
    count int
)

#创建预约订单/非预约订单统计目标表
create table if not exists app_didi.t_order_subscribe_total(
	order_date date,
    subscribe_name varchar(20),
    count int
)
```



#### P140、22-Sqoop数据导出-数据导出操作

##### 将Hive中的结果表导出到MySQL中

```bash
cd 

#导出订单总笔数表数据
bin/sqoop export \
 --connect  jdbc:mysql://hadoop1:3306app_didi \
 --username root \
 --password 123456
 --table t_order_total
 --export-dir /usr/hive/warehouse/app_didi.db/t_order_total/month=2020-04
 
 #导出预约订单/非预约订单统计数据
 bin/sqoop export \
 --connect  jdbc:mysql://hadoop1:3306app_didi \
 --username root \
 --password 123456
 --table t_order_subscribe_total
 --export-dir /usr/hive/warehouse/app_didi.db/t_order_subscribe_total/month=2020-04
 
```

#### P141、24-Superset数据可视化-Superset安装和启动

##### 安装

虚拟机中已经安装

##### 启动

```
superset run -h 192.168.1.100 -p 8099 --with-threads -- reload --debugger
```

##### 页面访问

```
http://192.168.1.100:8099
用户名 admin
密码 123456
```



#### P142、25-Superset数据可视化-Superset创建数据源

###### 介绍

在启动完Superset，我们可以让它连接MySQL数据库

###### 连接

菜单栏 Sources-Databases  打开Databases页面 右上角 + 加号

###### 添加数据库

- **database** 数据库连接名称

  ```bash
  mysql+pymysql://root:123456@192.168.1.100/app_didi?charset=utf8
  ```

- **URL**  连接路径

- **测试连接**



#### P143、26-Superset数据可视化-订单总笔数可视化

效果：大字报

- 1-选择表数据源
- 2-添加表连接
- 3-设置表连接相关参数
- 4-设置图表参数
  - 图标类型：数字
  - 时间范围：No Filter
  - 指标：SUM(count)
- 5-运行查询：添加图标名称

#### P144、27-Superset数据可视化-预约和非预约订单可视化

效果：饼状图

- 1-选择表数据源
- 2-添加表连接
- 3-设置表连接相关参数
- 4-设置图表参数
  - 图标类型：饼图
  - 时间范围：No Filter
  - 指标：SUM(count)
  - 分组：subscribe_name
  - Customize：圆环图
  - Customize：饼图配色方案
  - 图标的标题名称

#### P145、28-Superset数据可视化-不同时段订单可视化

效果：柱状图

- 1-选择表数据源
- 2-添加表连接
- 3-设置表连接相关参数
- 4-设置图表参数
  - 图标类型：BarChart柱状图
  - 时间范围：No Filter
  - 指标：SUM(count)
  - 序列：timerrange
  - Customize：配色方案
  - 图表的标题名称

#### P146、29-Superset数据可视化-不同地域订单可视化

效果：柱状图

- 1-选择表数据源
- 2-添加表连接
- 3-设置表连接相关参数
- 4-设置图表参数
  - 图标类型：BarChart柱状图
  - 时间范围：No Filter
  - 指标Y轴：SUM(count)
  - 序列(X轴)：timerrange
  - Customize：配色方案
  - Customize：排序条形栏
  - Customize：条形栏的值
  - 图表的标题名称：不同地域的订单

#### P147、30-Superset数据可视化-不同年龄段和不同时段订单可视化

效果：双环图/日暴图

- 1-选择表数据源
- 2-添加表连接
- 3-设置表连接相关参数
- 4-设置图表参数
  - 图标类型：Sunburst Chart
  - 时间范围：No Filter
  - 层级：age_range(里层) order_time_range(外层)
  - 指标：SUM(count)
  - Customize：配色方案

#### P148、31-Superset数据可视化-看板开发mp4

在实现完每个指标的可视化之后，我们可以将这些图表集中在一个看板上，便于分析查看

###### 实现步骤

- 1-创建看板

- 2-设置看板名称

  Title：

- 3-进入看板

  Edit Dashboard

- 4-编辑看板

- 5-选自定义图表

  拖入左侧编辑区

- 6-制作看板

- 7-调整看板位置